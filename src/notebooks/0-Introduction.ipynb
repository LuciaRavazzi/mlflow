{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MlFlow Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MlFlow offers multiple services:\n",
    "- **MlFlow Tracking** for logging models and training statistics, register and load models. Autologging can be enabled. \n",
    "- **MlFlow Tracking Server**: is a centralized HTTP server that allows you to access your experiments artifacts regardless of where you run your code.\n",
    "- **MlFlow Registry** for registering a model in the MLflow model registry and how to retrieve registered models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **Tracking Server**\n",
    "\n",
    "A *tracking server* is a system used to monitor and record data about processes, activities, or events; The MLflow Tracking Server is a component used to log, store, and retrieve experiment metadata such as parameters, metrics, models, and artifacts.\n",
    "\n",
    "MlFlow uses a HTTP server which needs to be defined by a host and a port. The **host** specifies the network interface (IP address) where the MLflow server will listen for incoming connection. By default, MLflow uses 127.0.0.1, which means the server is only accessible from the same machine (localhost). If you want the server to be accessible to other machines on your network, you can set it to 0.0.0.0, which binds it to all available network interfaces. The **port** defines the port number on which the MLflow Tracking Server runs.\n",
    "\n",
    "Furthermore, on remote deployments, which is recommended for production use cases, the tracking server will be on object store (S3, ADLS, GCS, etc.).\n",
    "\n",
    "In other words, we are working with another server, indeed it's possibile to query logged information by a post request.  \n",
    "\n",
    "To specify mlflow server configuration:\n",
    "\n",
    "mlflow server --host 0.0.0.0 --port 5000\n",
    "\n",
    "You can start a tutorial and log models, experiments without a tracking server set up. With this mode, your experiment data and artifacts are saved directly under your current directory. Then, you shoudl connect the notebook to tracking server.\n",
    "\n",
    "By default mlflow server is related to the port 5000.\n",
    "\n",
    "In this case, all IP within the network will be able to connect to the server. On the other side, for connection to the MlFlow UI through http://IP_DEL_SERVER:8080. \n",
    "\n",
    "mlflow server configuration can be defined: \n",
    "- Locally\n",
    "- By Databricks Managed Services with limited quota. You can explore the mlflow services by the Databricks Workspace or import the notebook within Databricks. \n",
    "- By Cloud managed services\n",
    "\n",
    "For example, considering Databricks, you can decide to run the notebook within the Databricks workspace or connect your notebook to Databricks using Perosnal Access Tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# connect notebooks to tracking server\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Backend Storage**\n",
    "\n",
    "MlFlow stores:\n",
    "-  *Backend Store*: metadata for runs and experiments, like th RUN ID, start and end time etc., parameters, metrics, tags. By default, MlFlow stored metadata into ./mlruns directory within local directory but some databases can be used as well. Configuration can be done setting MLFLOW_TRACKING_URI, mlflow.set_tracking_uri() or by CLI command --backend-store-uri. The storage can be te local file path, a mysql database, HTTP Server or the storage service of managed service.\n",
    "-  *Artifact store*: can be used for large file, such as model weights, images, model and datafiles. MLflow by default stores artifacts in local ./mlruns directory, but also supports various locations suitable for large data: Amazon S3, Azure Blob Storage, Google Cloud Storage, SFTP server, and NFS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experiment, runs and logged models**\n",
    "\n",
    "Signature is used to check the schema of data.\n",
    "\n",
    "- Start a new run under the Experiment defined: with mlflow.start_run()\n",
    "- Log hyperparameters/parameters/other values: **mlflow.log_params(params_dict)** for dictionary, **mlflow.log_param(\"param_name\", value)** \n",
    "- Log metric/KPI: **mlflow.log_metric(\"name_metric\", value)**\n",
    "- Artefacts such as output files or plots: **mlflow.log_artifact(\"path/to/artifact\")**\n",
    "- Set tag for run (not model): **mlflow.set_tag(\"tag\", \"description\")**\n",
    "- Define schema of data for X and y: infer_signature\n",
    "- Register the model: mlflow.pyfunc.log_model\n",
    "- Register data **dataset = mlflow.data.from_pandas(df, source=dataset_source_url)** and **mlflow.log_input(dataset, context = \"training\")**\n",
    "- Log text: **mlflow.log_text(\"name_text\", \"artifact_path\")**\n",
    "- Log trace: **mlflow.log_trace()** log a single span\n",
    "- Log table: **mlflow.log_table(data=table_dict)** to save a table in a dict format\n",
    "- Log notebook: **mlflow.log_artifact(\"my_notebook.py\")** after converting the notebook to a script\n",
    "\n",
    "These are related to the modeling step but they could be used for pre-processing and feature engineering. \n",
    "\n",
    "In order to record everything: \n",
    "- Log every dataset after each experiment done on top of original data.\n",
    "- Log code used as a text file\n",
    "- Log steps as parameters\n",
    "- Set and use a feature store like Feast which allow us to track feature lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train, test = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_x = train.drop([\"quality\"], axis=1).values\n",
    "train_y = train[[\"quality\"]].values.ravel()\n",
    "test_x = test.drop([\"quality\"], axis=1).values\n",
    "test_y = test[[\"quality\"]].values.ravel()\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, random_state=42\n",
    ")\n",
    "signature = infer_signature(train_x, train_y)\n",
    "\n",
    "def train_model(params, epochs, train_x, train_y, valid_x, valid_y, test_x, test_y):\n",
    "    # Define model architecture\n",
    "    mean = np.mean(train_x, axis=0)\n",
    "    var = np.var(train_x, axis=0)\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input([train_x.shape[1]]),\n",
    "            keras.layers.Normalization(mean=mean, variance=var),\n",
    "            keras.layers.Dense(64, activation=\"relu\"),\n",
    "            keras.layers.Dense(1),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(\n",
    "            learning_rate=params[\"lr\"], momentum=params[\"momentum\"]\n",
    "        ),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=[keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "\n",
    "    # Train model with MLflow tracking\n",
    "    with mlflow.start_run(nested=True):\n",
    "        model.fit(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            validation_data=(valid_x, valid_y),\n",
    "            epochs=epochs,\n",
    "            batch_size=64,\n",
    "        )\n",
    "        # Evaluate the model\n",
    "        eval_result = model.evaluate(valid_x, valid_y, batch_size=64)\n",
    "        eval_rmse = eval_result[1]\n",
    "\n",
    "        # Log parameters and results\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"eval_rmse\", eval_rmse)\n",
    "\n",
    "        # Log model\n",
    "        mlflow.tensorflow.log_model(model, \"model\", signature=signature)\n",
    "\n",
    "        return {\"loss\": eval_rmse, \"status\": STATUS_OK, \"model\": model}\n",
    "    \n",
    "def objective(params):\n",
    "    # MLflow will track the parameters and results for each run\n",
    "    result = train_model(\n",
    "        params,\n",
    "        epochs=3,\n",
    "        train_x=train_x,\n",
    "        train_y=train_y,\n",
    "        valid_x=valid_x,\n",
    "        valid_y=valid_y,\n",
    "        test_x=test_x,\n",
    "        test_y=test_y,\n",
    "    )\n",
    "    return result    \n",
    "\n",
    "space = {\n",
    "    \"lr\": hp.loguniform(\"lr\", np.log(1e-5), np.log(1e-1)),\n",
    "    \"momentum\": hp.uniform(\"momentum\", 0.0, 1.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/18 14:35:56 INFO mlflow.tracking.fluent: Experiment with name '/wine-quality' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3                                            \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m21s\u001b[0m 469ms/step - loss: 31.7113 - root_mean_squared_error: 5.6313\n",
      "\u001b[1m30/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.0382 - root_mean_squared_error: 2.8824    \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.9677 - root_mean_squared_error: 2.4974 - val_loss: 0.5623 - val_root_mean_squared_error: 0.7499\n",
      "\n",
      "Epoch 2/3                                            \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 0.5693 - root_mean_squared_error: 0.7545\n",
      "\u001b[1m25/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5731 - root_mean_squared_error: 0.7569 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5630 - root_mean_squared_error: 0.7503 - val_loss: 0.5215 - val_root_mean_squared_error: 0.7222\n",
      "\n",
      "Epoch 3/3                                            \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.7177 - root_mean_squared_error: 0.8471\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5252 - root_mean_squared_error: 0.7244 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5251 - root_mean_squared_error: 0.7243 - val_loss: 0.5204 - val_root_mean_squared_error: 0.7214\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.4200 - root_mean_squared_error: 0.6481\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4844 - root_mean_squared_error: 0.6954 \n",
      "\n",
      "沛 View run puzzled-mare-980 at: http://127.0.0.1:5000/#/experiments/853502062285188487/runs/c542ca62a24a454392c41f27727ffb21\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/853502062285188487\n",
      "\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m16s\u001b[0m 367ms/step - loss: 32.5090 - root_mean_squared_error: 5.7017\n",
      "\u001b[1m24/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.5766 - root_mean_squared_error: 5.6142   \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 27.0096 - root_mean_squared_error: 5.1717 - val_loss: 6.3967 - val_root_mean_squared_error: 2.5292\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 6.5565 - root_mean_squared_error: 2.5606\n",
      "\u001b[1m31/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9894 - root_mean_squared_error: 2.2307 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.6121 - root_mean_squared_error: 2.1418 - val_loss: 2.5790 - val_root_mean_squared_error: 1.6059\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 2.0228 - root_mean_squared_error: 1.4222\n",
      "\u001b[1m31/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4386 - root_mean_squared_error: 1.5607 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3683 - root_mean_squared_error: 1.5380 - val_loss: 2.0297 - val_root_mean_squared_error: 1.4247\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.8357 - root_mean_squared_error: 1.3549\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0389 - root_mean_squared_error: 1.4275 \n",
      "\n",
      "沛 View run fearless-gnat-742 at: http://127.0.0.1:5000/#/experiments/853502062285188487/runs/a9986e7369fd49e6a53029b3f7086381\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/853502062285188487  \n",
      "\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m15s\u001b[0m 348ms/step - loss: 35.9010 - root_mean_squared_error: 5.9917\n",
      "\u001b[1m34/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35.4627 - root_mean_squared_error: 5.9550   \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 35.1957 - root_mean_squared_error: 5.9324 - val_loss: 30.9140 - val_root_mean_squared_error: 5.5600\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 32.0649 - root_mean_squared_error: 5.6626\n",
      "\u001b[1m36/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29.7052 - root_mean_squared_error: 5.4495 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 29.2321 - root_mean_squared_error: 5.4055 - val_loss: 23.5761 - val_root_mean_squared_error: 4.8555\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 25.0482 - root_mean_squared_error: 5.0048\n",
      "\u001b[1m28/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23.0350 - root_mean_squared_error: 4.7987 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 22.2654 - root_mean_squared_error: 4.7170 - val_loss: 17.5040 - val_root_mean_squared_error: 4.1838\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 18.5113 - root_mean_squared_error: 4.3025\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17.4592 - root_mean_squared_error: 4.1782 \n",
      "\n",
      "沛 View run stylish-hog-553 at: http://127.0.0.1:5000/#/experiments/853502062285188487/runs/40ef123bcd3942d9b0c48ca20b35f277\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/853502062285188487  \n",
      "\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m16s\u001b[0m 372ms/step - loss: 28.0040 - root_mean_squared_error: 5.2919\n",
      "\u001b[1m22/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.0234 - root_mean_squared_error: 3.0700   \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 6.4671 - root_mean_squared_error: 2.4049 - val_loss: 0.5764 - val_root_mean_squared_error: 0.7592\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.5707 - root_mean_squared_error: 0.7554\n",
      "\u001b[1m31/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5435 - root_mean_squared_error: 0.7371 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5489 - root_mean_squared_error: 0.7408 - val_loss: 0.5292 - val_root_mean_squared_error: 0.7274\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 0.5417 - root_mean_squared_error: 0.7360\n",
      "\u001b[1m31/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5534 - root_mean_squared_error: 0.7438 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5505 - root_mean_squared_error: 0.7419 - val_loss: 0.5398 - val_root_mean_squared_error: 0.7347\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.4864 - root_mean_squared_error: 0.6974\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5075 - root_mean_squared_error: 0.7121 \n",
      "\n",
      "沛 View run amazing-zebra-654 at: http://127.0.0.1:5000/#/experiments/853502062285188487/runs/853c9503a28f4dab948218636c90d482\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/853502062285188487  \n",
      "\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m16s\u001b[0m 359ms/step - loss: 33.1855 - root_mean_squared_error: 5.7607\n",
      "\u001b[1m29/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29.7388 - root_mean_squared_error: 5.4497   \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 27.6525 - root_mean_squared_error: 5.2497 - val_loss: 13.6226 - val_root_mean_squared_error: 3.6909\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - loss: 12.8828 - root_mean_squared_error: 3.5893\n",
      "\u001b[1m29/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.8286 - root_mean_squared_error: 3.4367 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.9893 - root_mean_squared_error: 3.3091 - val_loss: 5.5792 - val_root_mean_squared_error: 2.3620\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 5.5367 - root_mean_squared_error: 2.3530\n",
      "\u001b[1m40/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9450 - root_mean_squared_error: 2.2219 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8387 - root_mean_squared_error: 2.1974 - val_loss: 3.1452 - val_root_mean_squared_error: 1.7735\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.0731 - root_mean_squared_error: 1.7530\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.0677 - root_mean_squared_error: 1.7514 \n",
      "\n",
      "沛 View run classy-crab-982 at: http://127.0.0.1:5000/#/experiments/853502062285188487/runs/02cb2123dc0549ed8772034673d24565\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/853502062285188487  \n",
      "\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m15s\u001b[0m 350ms/step - loss: 38.3507 - root_mean_squared_error: 6.1928\n",
      "\u001b[1m45/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.4283 - root_mean_squared_error: 3.0491   \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 10.1581 - root_mean_squared_error: 3.0053 - val_loss: 0.9811 - val_root_mean_squared_error: 0.9905\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.8984 - root_mean_squared_error: 0.9478\n",
      "\u001b[1m39/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8740 - root_mean_squared_error: 0.9344 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8587 - root_mean_squared_error: 0.9261 - val_loss: 0.6823 - val_root_mean_squared_error: 0.8260\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.5321 - root_mean_squared_error: 0.7294\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6108 - root_mean_squared_error: 0.7813 - val_loss: 0.5840 - val_root_mean_squared_error: 0.7642\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.4726 - root_mean_squared_error: 0.6875\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5622 - root_mean_squared_error: 0.7491 \n",
      "\n",
      "沛 View run suave-quail-595 at: http://127.0.0.1:5000/#/experiments/853502062285188487/runs/8c226908001b4b46acf45a059d79f38d\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/853502062285188487  \n",
      "\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m15s\u001b[0m 353ms/step - loss: 31.3140 - root_mean_squared_error: 5.5959\n",
      "\u001b[1m44/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 29.1736 - root_mean_squared_error: 5.4008   \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 29.0758 - root_mean_squared_error: 5.3916 - val_loss: 24.0161 - val_root_mean_squared_error: 4.9006\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 23.1060 - root_mean_squared_error: 4.8069\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22.5125 - root_mean_squared_error: 4.7441 - val_loss: 18.3517 - val_root_mean_squared_error: 4.2839\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 17.0174 - root_mean_squared_error: 4.1252\n",
      "\u001b[1m29/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17.7703 - root_mean_squared_error: 4.2152 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17.2925 - root_mean_squared_error: 4.1575 - val_loss: 13.8986 - val_root_mean_squared_error: 3.7281\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 14.1715 - root_mean_squared_error: 3.7645\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.7690 - root_mean_squared_error: 3.7106 \n",
      "\n",
      "沛 View run bedecked-grouse-691 at: http://127.0.0.1:5000/#/experiments/853502062285188487/runs/fbd1e8065b3c48078a8b4f5167095d38\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/853502062285188487  \n",
      "\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m17s\u001b[0m 390ms/step - loss: 30.1906 - root_mean_squared_error: 5.4946\n",
      "\u001b[1m33/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.4582 - root_mean_squared_error: 5.6087   \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 31.4524 - root_mean_squared_error: 5.6082 - val_loss: 31.3253 - val_root_mean_squared_error: 5.5969\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 30.8422 - root_mean_squared_error: 5.5536\n",
      "\u001b[1m39/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30.8827 - root_mean_squared_error: 5.5572 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 30.8810 - root_mean_squared_error: 5.5570 - val_loss: 30.7884 - val_root_mean_squared_error: 5.5487\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 29.3595 - root_mean_squared_error: 5.4184\n",
      "\u001b[1m22/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30.4201 - root_mean_squared_error: 5.5153 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 30.4224 - root_mean_squared_error: 5.5156 - val_loss: 30.2607 - val_root_mean_squared_error: 5.5010\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 30.9950 - root_mean_squared_error: 5.5673\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 30.3054 - root_mean_squared_error: 5.5050 \n",
      "\n",
      "沛 View run fun-frog-302 at: http://127.0.0.1:5000/#/experiments/853502062285188487/runs/55548f86abb049f29d7ced03a2054ae3\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/853502062285188487  \n",
      "\n",
      "100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 8/8 [02:24<00:00, 18.12s/trial, best loss: 0.7213677763938904]\n",
      "Best parameters: {'lr': np.float64(0.037339057078952395), 'momentum': np.float64(0.8549582030556139)}\n",
      "Best eval rmse: 0.7213677763938904\n",
      "沛 View run shivering-frog-95 at: http://127.0.0.1:5000/#/experiments/853502062285188487/runs/275c79d536664a0aa62c1bd8a534d64f\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/853502062285188487\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"/wine-quality\")\n",
    "with mlflow.start_run():\n",
    "    # Conduct the hyperparameter search using Hyperopt\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=8,\n",
    "        trials=trials,\n",
    "    )\n",
    "\n",
    "    # Fetch the details of the best run\n",
    "    best_run = sorted(trials.results, key=lambda x: x[\"loss\"])[0]\n",
    "\n",
    "    # Log the best parameters, loss, and model\n",
    "    mlflow.log_params(best)\n",
    "    mlflow.log_metric(\"eval_rmse\", best_run[\"loss\"])\n",
    "    mlflow.tensorflow.log_model(best_run[\"model\"], \"model\", signature=signature)\n",
    "\n",
    "    # Print out the best parameters and corresponding loss\n",
    "    print(f\"Best parameters: {best}\")\n",
    "    print(f\"Best eval rmse: {best_run['loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Register and Load a Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can register a model:\n",
    "- Manually using the MlFlow UI\n",
    "- By using API mlflow.log_model(registered_model_name=<model_name>) while logging the model. If the model exists, a new version of that model will be created. \n",
    "- By using API mlflow.register_model(<model_uri>, <model_name>) after logging the model\n",
    "\n",
    "Moreover, *aliases* and *tags* can be attached to the registered model. Model Registry is integrated into the Tracking server ecosystem. \n",
    "\n",
    "You can potentially define the localhost of the model registry: mlflow.set_registry_uri(\"uc:http://localhost:8080\") for Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'prova-dopo-logging'.\n",
      "2025/03/18 17:48:36 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: prova-dopo-logging, version 1\n",
      "Created version '1' of model 'prova-dopo-logging'.\n"
     ]
    }
   ],
   "source": [
    "result = mlflow.register_model(\n",
    "    \"runs:/853c9503a28f4dab948218636c90d482/amazing-zebra-654\", \"prova-dopo-logging\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RestException",
     "evalue": "RESOURCE_ALREADY_EXISTS: Registered Model (name=okay) already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRestException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MlflowClient\n\u001b[0;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m MlflowClient()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename_registered_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprova-dopo-logging\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mokay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ravazziluc\\AppData\\Local\\anaconda3\\envs\\tutorial\\lib\\site-packages\\mlflow\\tracking\\client.py:3617\u001b[0m, in \u001b[0;36mMlflowClient.rename_registered_model\u001b[1;34m(self, name, new_name)\u001b[0m\n\u001b[0;32m   3567\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update registered model name.\u001b[39;00m\n\u001b[0;32m   3568\u001b[0m \n\u001b[0;32m   3569\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3614\u001b[0m \u001b[38;5;124;03m    description: This sentiment analysis model classifies the tone-happy, sad, angry.\u001b[39;00m\n\u001b[0;32m   3615\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3616\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_prompt(name)\n\u001b[1;32m-> 3617\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_registry_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename_registered_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ravazziluc\\AppData\\Local\\anaconda3\\envs\\tutorial\\lib\\site-packages\\mlflow\\tracking\\_model_registry\\client.py:98\u001b[0m, in \u001b[0;36mModelRegistryClient.rename_registered_model\u001b[1;34m(self, name, new_name)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_name\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe name must not be an empty string.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename_registered_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ravazziluc\\AppData\\Local\\anaconda3\\envs\\tutorial\\lib\\site-packages\\mlflow\\store\\model_registry\\rest_store.py:113\u001b[0m, in \u001b[0;36mRestStore.rename_registered_model\u001b[1;34m(self, name, new_name)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03mRename the registered model.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    112\u001b[0m req_body \u001b[38;5;241m=\u001b[39m message_to_json(RenameRegisteredModel(name\u001b[38;5;241m=\u001b[39mname, new_name\u001b[38;5;241m=\u001b[39mnew_name))\n\u001b[1;32m--> 113\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRenameRegisteredModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RegisteredModel\u001b[38;5;241m.\u001b[39mfrom_proto(response_proto\u001b[38;5;241m.\u001b[39mregistered_model)\n",
      "File \u001b[1;32mc:\\Users\\ravazziluc\\AppData\\Local\\anaconda3\\envs\\tutorial\\lib\\site-packages\\mlflow\\store\\model_registry\\base_rest_store.py:44\u001b[0m, in \u001b[0;36mBaseRestStore._call_endpoint\u001b[1;34m(self, api, json_body, call_all_endpoints, extra_headers)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     endpoint, method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_endpoint_from_method(api)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_headers\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ravazziluc\\AppData\\Local\\anaconda3\\envs\\tutorial\\lib\\site-packages\\mlflow\\utils\\rest_utils.py:392\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[1;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[0;32m    389\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n\u001b[0;32m    390\u001b[0m     response \u001b[38;5;241m=\u001b[39m http_request(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs)\n\u001b[1;32m--> 392\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mverify_rest_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m response_to_parse \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    394\u001b[0m js_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response_to_parse)\n",
      "File \u001b[1;32mc:\\Users\\ravazziluc\\AppData\\Local\\anaconda3\\envs\\tutorial\\lib\\site-packages\\mlflow\\utils\\rest_utils.py:249\u001b[0m, in \u001b[0;36mverify_rest_response\u001b[1;34m(response, endpoint)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _can_parse_as_json_object(response\u001b[38;5;241m.\u001b[39mtext):\n\u001b[1;32m--> 249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m RestException(json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mtext))\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m         base_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI request to endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed with error code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != 200\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m         )\n",
      "\u001b[1;31mRestException\u001b[0m: RESOURCE_ALREADY_EXISTS: Registered Model (name=okay) already exists."
     ]
    }
   ],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "client.rename_registered_model(\n",
    "    name=\"prova-dopo-logging\",\n",
    "    new_name=\"okay\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Alias**\n",
    "\n",
    "It can replace the version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=['champion'], creation_timestamp=1742316516065, current_stage='None', description='', last_updated_timestamp=1742316516065, name='prova-dopo-logging', run_id='853c9503a28f4dab948218636c90d482', run_link='', source='mlflow-artifacts:/853502062285188487/853c9503a28f4dab948218636c90d482/artifacts/amazing-zebra-654', status='READY', status_message=None, tags={}, user_id='', version='1'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Assign this alias to that model with that version\n",
    "client.set_registered_model_alias(\"prova-dopo-logging\", \"champion\", 1)\n",
    "\n",
    "# Gte model version by alias\n",
    "client.get_model_version_by_alias(\"prova-dopo-logging\", \"champion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ravazziluc\\AppData\\Local\\anaconda3\\envs\\tutorial\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "model_name = \"prova-dopo-logging\"\n",
    "alias = \"champion\"\n",
    "\n",
    "champion_version = mlflow.pyfunc.load_model(f\"models:/{model_name}@{alias}\")\n",
    "\n",
    "champion_version.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Tag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Assign this alias to that model with that version\n",
    "client.set_registered_model_tag(\"prova-dopo-logging\", \"task\", \"regression\")\n",
    "\n",
    "# Set model version tag\n",
    "client.set_model_version_tag(\"prova-dopo-logging\", \"1\", \"validation_status\", \"approved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a registered model:\n",
    "- mlflow.pyfunc.load_model(f\"runs:/{mlflow_run_id}/{run_relative_path_to_model}\")\n",
    "- mlflow.pyfunc.load_model(f\"models:/{model_name}/{model_version}\")\n",
    "- mlflow.pyfunc.load_model(f\"models:/{model_name}@{model_version_alias}\")\n",
    "\n",
    "Furthermore, on remote deployments, which is recommended for production use cases, the model registry will be on a relational database (PostgreSQL, MySQL, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 7/7 [00:00<00:00, 103.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "[[1.9549971]\n",
      " [4.91389  ]\n",
      " [3.1608555]\n",
      " ...\n",
      " [3.0928452]\n",
      " [4.0647297]\n",
      " [2.1002364]]\n"
     ]
    }
   ],
   "source": [
    "import mlflow.sklearn\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Load the model from the Model Registry\n",
    "model_uri = f\"models:/wine-quality-best/1\"\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Generate a new dataset for prediction and predict\n",
    "y_pred_new = model.predict(test_x)\n",
    "\n",
    "print(y_pred_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Serving models**\n",
    "\n",
    "After registering a model, it can be served in order to be queried:\n",
    "\n",
    "mlflow models serve -m \"models:/wine-quality-best/1\" --port 5002 --no-conda\n",
    "\n",
    "It's better to run the tracking and serving in different machines for resources limitations within a production environment. \n",
    "Then, the model can be queried through a REST API request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [[5.43813943862915]]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the MLflow serving endpoint\n",
    "url = \"http://localhost:5002/invocations\"\n",
    "\n",
    "# Define input data (modify based on your model's expected format)\n",
    "payload = {\"instances\": [[7,0.27,0.36,20.7,0.045,45,170,1.001,3,0.45,8.8]]}  # Example input\n",
    "\n",
    "# Set headers for JSON content\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Send request\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# Print response\n",
    "print(response.json())  # Parsed JSON response from model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Serve with docker container**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After regustering the model, build an image for Docker for that model:\n",
    "\n",
    "mlflow models build-docker --model-uri \"models:/wine-quality/1\" --name \"qs_mlops\"\n",
    "\n",
    "And running: \n",
    "\n",
    "docker run -p 5002:8080 qs_mlops\n",
    "\n",
    "So, the model can be queried by REST API requests. \n",
    "\n",
    "In this case, you can collect the model and its dependencies, collapse everything into the docker image, push the image to a shared docker registry and finally, your collegues can pull the image and query the model. \n",
    "\n",
    "Regarding managed services, you can use docker images or use built-in suppport for Mlflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MlFlow server can be used in different ways:\n",
    "    \n",
    "- Locally: more secure for data\n",
    "- Databricks Free Trial: leverages on Databricks platform for MlFlow functionalities\n",
    "- Hosted Tracking service: managed solutions   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **How can we use mlflow in a team?**\n",
    "\n",
    "First of all, a tracking server must be set on a server or in the cloud:\n",
    "\n",
    "mlflow server \\\n",
    "    --backend-store-url postgresql://user:password@db-host/mlflow_db \\\n",
    "    --default-artifact-root s3://your-bucket/mlflow-artifacts \\\n",
    "    --host 0.0.0.0\n",
    "    --port 5000\n",
    "\n",
    "In this way, metadata are stored in a relational database and artifact in a S3 bucket. Moreover, each server within a network can access to the tracking server. \n",
    "\n",
    "Then, the notebooks must be attached to the tracking server through the tracking URI http://<server-ip>:5000. \n",
    "\n",
    "On the same server, you can deploy a model. So, you can deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset**\n",
    "\n",
    "The run information can collect data characteristics such as data schema, the url of the data source etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ravazziluc\\AppData\\Local\\anaconda3\\envs\\tutorial\\lib\\site-packages\\mlflow\\data\\dataset_source_registry.py:149: UserWarning: Failed to determine whether UCVolumeDatasetSource can resolve source information for 'https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv'. Exception: \n",
      "  return _dataset_source_registry.resolve(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沛 View run rogue-shark-859 at: http://127.0.0.1:5000/#/experiments/0/runs/315e787ae8d54795abdaf8b7300abc9d\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ravazziluc\\AppData\\Local\\anaconda3\\envs\\tutorial\\lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow.data\n",
    "import pandas as pd\n",
    "from mlflow.data.pandas_dataset import PandasDataset\n",
    "\n",
    "dataset_source_url = \"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv\"\n",
    "raw_data = pd.read_csv(dataset_source_url, delimiter=\";\")\n",
    "\n",
    "# Create an instance of a PandasDataset\n",
    "dataset = mlflow.data.from_pandas(\n",
    "    raw_data, source=dataset_source_url, name=\"wine quality - white\", targets=\"quality\"\n",
    ")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_input(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlflow.data.http_dataset_source.HTTPDatasetSource at 0x219942451f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.data.get_source(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
