{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MlFlow Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MlFlow offers multiple services:\n",
    "- **MlFlow Tracking** for logging models and training statistics, register and load models. Autologging can be enabled. \n",
    "- **MlFlow Tracking Server**: is a centralized HTTP server that allows you to access your experiments artifacts regardless of where you run your code.\n",
    "- **MlFlow Registry** for registering a model in the MLflow model registry and how to retrieve registered models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **Tracking Server**\n",
    "\n",
    "A *tracking server* is a system used to monitor and record data about processes, activities, or events; The MLflow Tracking Server is a component used to log, store, and retrieve experiment metadata such as parameters, metrics, models, and artifacts.\n",
    "\n",
    "MlFlow uses a HTTP server which needs to be defined by a host and a port. The **host** specifies the network interface (IP address) where the MLflow server will listen for incoming connection. By default, MLflow uses 127.0.0.1, which means the server is only accessible from the same machine (localhost). If you want the server to be accessible to other machines on your network, you can set it to 0.0.0.0, which binds it to all available network interfaces. The **port** defines the port number on which the MLflow Tracking Server runs.\n",
    "\n",
    "Furthermore, on remote deployments, which is recommended for production use cases, the tracking server will be on object store (S3, ADLS, GCS, etc.).\n",
    "\n",
    "In other words, we are working with another server, indeed it's possibile to query logged information by a post request.  \n",
    "\n",
    "To specify mlflow server configuration:\n",
    "\n",
    "mlflow server --host 0.0.0.0 --port 5000\n",
    "\n",
    "You can start a tutorial and log models, experiments without a tracking server set up. With this mode, your experiment data and artifacts are saved directly under your current directory. Then, you shoudl connect the notebook to tracking server.\n",
    "\n",
    "By default mlflow server is related to the port 5000.\n",
    "\n",
    "In this case, all IP within the network will be able to connect to the server. On the other side, for connection to the MlFlow UI through http://IP_DEL_SERVER:8080. \n",
    "\n",
    "mlflow server configuration can be defined: \n",
    "- Locally\n",
    "- By Databricks Managed Services with limited quota. You can explore the mlflow services by the Databricks Workspace or import the notebook within Databricks. \n",
    "- By Cloud managed services\n",
    "\n",
    "For example, considering Databricks, you can decide to run the notebook within the Databricks workspace or connect your notebook to Databricks using Perosnal Access Tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# connect notebooks to tracking server\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Backend Storage**\n",
    "\n",
    "MlFlow stores:\n",
    "-  *Backend Store*: metadata for runs and experiments, like th RUN ID, start and end time etc., parameters, metrics, tags. By default, MlFlow stored metadata into ./mlruns directory within local directory but some databases can be used as well. Configuration can be done setting MLFLOW_TRACKING_URI, mlflow.set_tracking_uri() or by CLI command --backend-store-uri. The storage can be te local file path, a mysql database, HTTP Server or the storage service of managed service.\n",
    "-  *Artifact store*: can be used for large file, such as model weights, images, model and datafiles. MLflow by default stores artifacts in local ./mlruns directory, but also supports various locations suitable for large data: Amazon S3, Azure Blob Storage, Google Cloud Storage, SFTP server, and NFS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experiment and runs**\n",
    "\n",
    "Signature is used to check the schema of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train, test = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_x = train.drop([\"quality\"], axis=1).values\n",
    "train_y = train[[\"quality\"]].values.ravel()\n",
    "test_x = test.drop([\"quality\"], axis=1).values\n",
    "test_y = test[[\"quality\"]].values.ravel()\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, random_state=42\n",
    ")\n",
    "signature = infer_signature(train_x, train_y)\n",
    "\n",
    "def train_model(params, epochs, train_x, train_y, valid_x, valid_y, test_x, test_y):\n",
    "    # Define model architecture\n",
    "    mean = np.mean(train_x, axis=0)\n",
    "    var = np.var(train_x, axis=0)\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input([train_x.shape[1]]),\n",
    "            keras.layers.Normalization(mean=mean, variance=var),\n",
    "            keras.layers.Dense(64, activation=\"relu\"),\n",
    "            keras.layers.Dense(1),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(\n",
    "            learning_rate=params[\"lr\"], momentum=params[\"momentum\"]\n",
    "        ),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=[keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "\n",
    "    # Train model with MLflow tracking\n",
    "    with mlflow.start_run(nested=True):\n",
    "        model.fit(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            validation_data=(valid_x, valid_y),\n",
    "            epochs=epochs,\n",
    "            batch_size=64,\n",
    "        )\n",
    "        # Evaluate the model\n",
    "        eval_result = model.evaluate(valid_x, valid_y, batch_size=64)\n",
    "        eval_rmse = eval_result[1]\n",
    "\n",
    "        # Log parameters and results\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"eval_rmse\", eval_rmse)\n",
    "\n",
    "        # Log model\n",
    "        mlflow.tensorflow.log_model(model, \"model\", signature=signature)\n",
    "\n",
    "        return {\"loss\": eval_rmse, \"status\": STATUS_OK, \"model\": model}\n",
    "    \n",
    "def objective(params):\n",
    "    # MLflow will track the parameters and results for each run\n",
    "    result = train_model(\n",
    "        params,\n",
    "        epochs=3,\n",
    "        train_x=train_x,\n",
    "        train_y=train_y,\n",
    "        valid_x=valid_x,\n",
    "        valid_y=valid_y,\n",
    "        test_x=test_x,\n",
    "        test_y=test_y,\n",
    "    )\n",
    "    return result    \n",
    "\n",
    "space = {\n",
    "    \"lr\": hp.loguniform(\"lr\", np.log(1e-5), np.log(1e-1)),\n",
    "    \"momentum\": hp.uniform(\"momentum\", 0.0, 1.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/18 11:25:01 INFO mlflow.tracking.fluent: Experiment with name '/wine-quality' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3                                            \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m5:19\u001b[0m 7s/step - loss: 43.6150 - root_mean_squared_error: 6.6042\n",
      "\u001b[1m34/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42.1192 - root_mean_squared_error: 6.4898 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 41.9426 - root_mean_squared_error: 6.4762 - val_loss: 41.3060 - val_root_mean_squared_error: 6.4270\n",
      "\n",
      "Epoch 2/3                                            \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 43.4453 - root_mean_squared_error: 6.5913\n",
      "\u001b[1m39/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40.8346 - root_mean_squared_error: 6.3900 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 40.7671 - root_mean_squared_error: 6.3848 - val_loss: 40.3009 - val_root_mean_squared_error: 6.3483\n",
      "\n",
      "Epoch 3/3                                            \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 39.6990 - root_mean_squared_error: 6.3007\n",
      "\u001b[1m28/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39.8172 - root_mean_squared_error: 6.3101 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 39.6989 - root_mean_squared_error: 6.3007 - val_loss: 39.3249 - val_root_mean_squared_error: 6.2710\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 39.9512 - root_mean_squared_error: 6.3207\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 39.1336 - root_mean_squared_error: 6.2556 \n",
      "\n",
      "沛 View run defiant-dog-403 at: http://127.0.0.1:5000/#/experiments/650760020222606884/runs/6fdc1278535641d2a09df604ef45aaaf\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/650760020222606884\n",
      "\n",
      "Epoch 1/3                                                                     \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m13s\u001b[0m 310ms/step - loss: 35.9100 - root_mean_squared_error: 5.9925\n",
      "\u001b[1m25/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.9488 - root_mean_squared_error: 4.0148   \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 12.0777 - root_mean_squared_error: 3.3300 - val_loss: 1.6245 - val_root_mean_squared_error: 1.2746\n",
      "\n",
      "Epoch 2/3                                                                     \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 1.5482 - root_mean_squared_error: 1.2443\n",
      "\u001b[1m44/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3748 - root_mean_squared_error: 1.1724 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3723 - root_mean_squared_error: 1.1713 - val_loss: 1.2359 - val_root_mean_squared_error: 1.1117\n",
      "\n",
      "Epoch 3/3                                                                     \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1.4154 - root_mean_squared_error: 1.1897\n",
      "\u001b[1m39/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1069 - root_mean_squared_error: 1.0516 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0992 - root_mean_squared_error: 1.0479 - val_loss: 1.0030 - val_root_mean_squared_error: 1.0015\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.9221 - root_mean_squared_error: 0.9603\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0217 - root_mean_squared_error: 1.0105 \n",
      "\n",
      "沛 View run kindly-stag-392 at: http://127.0.0.1:5000/#/experiments/650760020222606884/runs/ffe8b46cee784b30b9530a444ef8d6a1\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/650760020222606884 \n",
      "\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m19s\u001b[0m 433ms/step - loss: 36.9334 - root_mean_squared_error: 6.0773\n",
      "\u001b[1m37/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.3409 - root_mean_squared_error: 2.3422    \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 5.4796 - root_mean_squared_error: 2.1660 - val_loss: 0.7076 - val_root_mean_squared_error: 0.8412\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.8238 - root_mean_squared_error: 0.9076\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6356 - root_mean_squared_error: 0.7971 - val_loss: 0.5430 - val_root_mean_squared_error: 0.7369\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.5918 - root_mean_squared_error: 0.7693\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5891 - root_mean_squared_error: 0.7674 - val_loss: 0.5577 - val_root_mean_squared_error: 0.7468\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5389 - root_mean_squared_error: 0.7341\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5468 - root_mean_squared_error: 0.7392 \n",
      "\n",
      "沛 View run amusing-mare-852 at: http://127.0.0.1:5000/#/experiments/650760020222606884/runs/0c919e5ab0d44a2f8baed7ac814003ff\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/650760020222606884  \n",
      "\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m27s\u001b[0m 606ms/step - loss: 40.1068 - root_mean_squared_error: 6.3330\n",
      "\u001b[1m36/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 30.2238 - root_mean_squared_error: 5.4786   \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 28.1122 - root_mean_squared_error: 5.2738 - val_loss: 8.0294 - val_root_mean_squared_error: 2.8336\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 7.8132 - root_mean_squared_error: 2.7952\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9159 - root_mean_squared_error: 2.4246 - val_loss: 2.9468 - val_root_mean_squared_error: 1.7166\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 2.4510 - root_mean_squared_error: 1.5656\n",
      "\u001b[1m44/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4979 - root_mean_squared_error: 1.5802 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4911 - root_mean_squared_error: 1.5780 - val_loss: 2.2521 - val_root_mean_squared_error: 1.5007\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.9478 - root_mean_squared_error: 1.3956\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2223 - root_mean_squared_error: 1.4901 \n",
      "\n",
      "沛 View run capable-ape-526 at: http://127.0.0.1:5000/#/experiments/650760020222606884/runs/a44bc221e5f041b59647bb39e4a77447\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/650760020222606884  \n",
      "\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m15s\u001b[0m 351ms/step - loss: 40.1536 - root_mean_squared_error: 6.3367\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 38.1966 - root_mean_squared_error: 6.1798   \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 38.1609 - root_mean_squared_error: 6.1769 - val_loss: 33.9234 - val_root_mean_squared_error: 5.8244\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 36.8231 - root_mean_squared_error: 6.0682\n",
      "\u001b[1m42/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32.8023 - root_mean_squared_error: 5.7264 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 32.6096 - root_mean_squared_error: 5.7094 - val_loss: 28.7946 - val_root_mean_squared_error: 5.3661\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 27.6159 - root_mean_squared_error: 5.2551\n",
      "\u001b[1m40/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 27.6133 - root_mean_squared_error: 5.2546 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 27.4348 - root_mean_squared_error: 5.2374 - val_loss: 24.4461 - val_root_mean_squared_error: 4.9443\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 24.6393 - root_mean_squared_error: 4.9638\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 24.1383 - root_mean_squared_error: 4.9130 \n",
      "\n",
      "沛 View run abrasive-panda-440 at: http://127.0.0.1:5000/#/experiments/650760020222606884/runs/883a6f46977248a185f727a3ae20daa3\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/650760020222606884  \n",
      "\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m18s\u001b[0m 422ms/step - loss: 33.4140 - root_mean_squared_error: 5.7805\n",
      "\u001b[1m44/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏―u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 26.6838 - root_mean_squared_error: 5.1550   \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 26.3351 - root_mean_squared_error: 5.1200 - val_loss: 12.5271 - val_root_mean_squared_error: 3.5394\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 14.1511 - root_mean_squared_error: 3.7618\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.1045 - root_mean_squared_error: 3.1717 - val_loss: 5.3559 - val_root_mean_squared_error: 2.3143\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 4.6857 - root_mean_squared_error: 2.1647\n",
      "\u001b[1m28/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6250 - root_mean_squared_error: 2.1500 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.4651 - root_mean_squared_error: 2.1122 - val_loss: 3.2739 - val_root_mean_squared_error: 1.8094\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.3510 - root_mean_squared_error: 1.8306\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.3363 - root_mean_squared_error: 1.8264 \n",
      "\n",
      "沛 View run dazzling-shoat-440 at: http://127.0.0.1:5000/#/experiments/650760020222606884/runs/0007e79f2fe7485cb3d7a38d009c0809\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/650760020222606884  \n",
      "\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m14s\u001b[0m 333ms/step - loss: 37.7720 - root_mean_squared_error: 6.1459\n",
      "\u001b[1m32/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34.3824 - root_mean_squared_error: 5.8610   \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 32.6386 - root_mean_squared_error: 5.7065 - val_loss: 17.5243 - val_root_mean_squared_error: 4.1862\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 18.8605 - root_mean_squared_error: 4.3429\n",
      "\u001b[1m34/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.2489 - root_mean_squared_error: 3.8997 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.4374 - root_mean_squared_error: 3.7916 - val_loss: 7.5005 - val_root_mean_squared_error: 2.7387\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 7.2828 - root_mean_squared_error: 2.6987\n",
      "\u001b[1m41/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[37m笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.7547 - root_mean_squared_error: 2.5951 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.6007 - root_mean_squared_error: 2.5645 - val_loss: 3.9756 - val_root_mean_squared_error: 1.9939\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.8102 - root_mean_squared_error: 1.9520\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8991 - root_mean_squared_error: 1.9745 \n",
      "\n",
      "沛 View run stately-rook-381 at: http://127.0.0.1:5000/#/experiments/650760020222606884/runs/89423666ef724cb1ae7538b1f0f82b14\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/650760020222606884  \n",
      "\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m16s\u001b[0m 377ms/step - loss: 30.3206 - root_mean_squared_error: 5.5064\n",
      "\u001b[1m28/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30.4476 - root_mean_squared_error: 5.5176   \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 29.6049 - root_mean_squared_error: 5.4400 - val_loss: 23.5224 - val_root_mean_squared_error: 4.8500\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 23.7556 - root_mean_squared_error: 4.8740\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 21.4232 - root_mean_squared_error: 4.6274 - val_loss: 17.0087 - val_root_mean_squared_error: 4.1242\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 17.5203 - root_mean_squared_error: 4.1857\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.5503 - root_mean_squared_error: 3.9419 - val_loss: 12.2325 - val_root_mean_squared_error: 3.4975\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 12.5309 - root_mean_squared_error: 3.5399\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.1444 - root_mean_squared_error: 3.4848 \n",
      "\n",
      "沛 View run skittish-ant-942 at: http://127.0.0.1:5000/#/experiments/650760020222606884/runs/c589f877977e40c2a6a559c4c8f52e03\n",
      "\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/650760020222606884  \n",
      "\n",
      "100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 8/8 [02:32<00:00, 19.03s/trial, best loss: 0.7467827200889587]\n",
      "Best parameters: {'lr': np.float64(0.09724613987347985), 'momentum': np.float64(0.3102637408512099)}\n",
      "Best eval rmse: 0.7467827200889587\n",
      "沛 View run worried-ram-126 at: http://127.0.0.1:5000/#/experiments/650760020222606884/runs/4d9c7f85e7bc4c5a84aa6311e2c6a6aa\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/650760020222606884\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"/wine-quality\")\n",
    "with mlflow.start_run():\n",
    "    # Conduct the hyperparameter search using Hyperopt\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=8,\n",
    "        trials=trials,\n",
    "    )\n",
    "\n",
    "    # Fetch the details of the best run\n",
    "    best_run = sorted(trials.results, key=lambda x: x[\"loss\"])[0]\n",
    "\n",
    "    # Log the best parameters, loss, and model\n",
    "    mlflow.log_params(best)\n",
    "    mlflow.log_metric(\"eval_rmse\", best_run[\"loss\"])\n",
    "    mlflow.tensorflow.log_model(best_run[\"model\"], \"model\", signature=signature)\n",
    "\n",
    "    # Print out the best parameters and corresponding loss\n",
    "    print(f\"Best parameters: {best}\")\n",
    "    print(f\"Best eval rmse: {best_run['loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Register and Load a Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can register a model:\n",
    "- Manually using the MlFlow UI\n",
    "- By using API mlflow.log_model \n",
    "\n",
    "Moreover, *aliases* and *tags* can be attached to the registered model. Model Registry is integrated into the Tracking server ecosystem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a registered model:\n",
    "- mlflow.pyfunc.load_model(f\"runs:/{mlflow_run_id}/{run_relative_path_to_model}\")\n",
    "- mlflow.pyfunc.load_model(f\"models:/{model_name}/{model_version}\")\n",
    "- mlflow.pyfunc.load_model(f\"models:/{model_name}@{model_version_alias}\")\n",
    "\n",
    "Furthermore, on remote deployments, which is recommended for production use cases, the model registry will be on a relational database (PostgreSQL, MySQL, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 7/7 [00:00<00:00, 103.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "[[1.9549971]\n",
      " [4.91389  ]\n",
      " [3.1608555]\n",
      " ...\n",
      " [3.0928452]\n",
      " [4.0647297]\n",
      " [2.1002364]]\n"
     ]
    }
   ],
   "source": [
    "import mlflow.sklearn\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Load the model from the Model Registry\n",
    "model_uri = f\"models:/wine-quality-best/1\"\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Generate a new dataset for prediction and predict\n",
    "y_pred_new = model.predict(test_x)\n",
    "\n",
    "print(y_pred_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Serving models**\n",
    "\n",
    "After registering a model, it can be served in order to be queried:\n",
    "\n",
    "mlflow models serve -m \"models:/wine-quality-best/1\" --port 5002 --no-conda\n",
    "\n",
    "It's better to run the tracking and serving in different machines for resources limitations within a production environment. \n",
    "Then, the model can be queried through a REST API request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [[5.43813943862915]]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the MLflow serving endpoint\n",
    "url = \"http://localhost:5002/invocations\"\n",
    "\n",
    "# Define input data (modify based on your model's expected format)\n",
    "payload = {\"instances\": [[7,0.27,0.36,20.7,0.045,45,170,1.001,3,0.45,8.8]]}  # Example input\n",
    "\n",
    "# Set headers for JSON content\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Send request\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# Print response\n",
    "print(response.json())  # Parsed JSON response from model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Serve with docker container**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After regustering the model, build an image for Docker for that model:\n",
    "\n",
    "mlflow models build-docker --model-uri \"models:/wine-quality/1\" --name \"qs_mlops\"\n",
    "\n",
    "And running: \n",
    "\n",
    "docker run -p 5002:8080 qs_mlops\n",
    "\n",
    "So, the model can be queried by REST API requests. \n",
    "\n",
    "In this case, you can collect the model and its dependencies, collapse everything into the docker image, push the image to a shared docker registry and finally, your collegues can pull the image and query the model. \n",
    "\n",
    "Regarding managed services, you can use docker images or use built-in suppport for Mlflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MlFlow server can be used in different ways:\n",
    "    \n",
    "- Locally: more secure for data\n",
    "- Databricks Free Trial: leverages on Databricks platform for MlFlow functionalities\n",
    "- Hosted Tracking service: managed solutions   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **How can we use mlflow in a team?**\n",
    "\n",
    "First of all, a tracking server must be set on a server or in the cloud:\n",
    "\n",
    "mlflow server \\\n",
    "    --backend-store-url postgresql://user:password@db-host/mlflow_db \\\n",
    "    --default-artifact-root s3://your-bucket/mlflow-artifacts \\\n",
    "    --host 0.0.0.0\n",
    "    --port 5000\n",
    "\n",
    "In this way, metadata are stored in a relational database and artifact in a S3 bucket. Moreover, each server within a network can access to the tracking server. \n",
    "\n",
    "Then, the notebooks must be attached to the tracking server through the tracking URI http://<server-ip>:5000. \n",
    "\n",
    "On the same server, you can deploy a model. So, you can deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
